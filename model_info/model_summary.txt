model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   	Gradients
=================================================================
conv2d (Conv2D)              (None, 32, 32, 32)        896       	[0,1]
_________________________________________________________________
batch_normalization (BatchNo (None, 32, 32, 32)        128       	[2,3,4,5]
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      	[6,7]
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       	[8,9,10,11]
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         
_________________________________________________________________
dropout (Dropout)            (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     	[12,13]
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 64)        256       	[14,15,16,17]
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     	[18,19]
_________________________________________________________________	
batch_normalization_3 (Batch (None, 16, 16, 64)        256       	[20,21,22,23]
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     	[24,25]
_________________________________________________________________
batch_normalization_4 (Batch (None, 8, 8, 128)         512       	[26,27,28,29]
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    	[30,31]
_________________________________________________________________
batch_normalization_5 (Batch (None, 8, 8, 128)         512       	[32,33,34,35]
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4, 4, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 2048)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               262272    	[36,37]
_________________________________________________________________
batch_normalization_6 (Batch (None, 128)               512       	[38,39,40,41]
_________________________________________________________________
dropout_3 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1290      	[42,43]
=================================================================
Total params: 552,874
Trainable params: 551,722
Non-trainable params: 1,152
_________________________________________________________________

thresholding_spars, [-th1,th1]

Two kinds of layers:
[0,6,12,24,30,36,42]

others: keep 

sparsification only acc

https://www.overleaf.com/2746885395tgxmmtxhznyq

https://www.overleaf.com/2684296277hfrsmywyvxsg	: old version

https://proceedings.mlr.press/v119/fu20c.html


552874 * 0.4 = 221150 (how many gradients are zeroed)

[6,12,24,30,36,42] = 9216 + 18432 + 36854 + 73728 + 147458 + 262144 + 1280 = 549112
221150/549112 = 40.2%

60% vs 59.8%




